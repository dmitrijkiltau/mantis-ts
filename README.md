# MANTIS

_Minimal Adaptive Neural Tool-Integrated System_ is a versatile AI assistant designed to be a user-friendly interface for various AI models and tools. It aims to simplify interactions with AI technologies, making them accessible to a broader audience.

## Design Principles

- Deterministic over creative
- Contracts over prompts
- Failure is explicit
- Small models decide, larger models execute
- Retries are bounded

## Orchestrator

`assistant/src/orchestrator.ts` contains an `Orchestrator` class that renders contract prompts and exposes the associated validators so the decision logic described in `ARCHITECTURE` can be exercised programmatically.

The bridge between the orchestrator and the contracts lives in the `ContractPrompt` envelope, which bundles the prompt text, the target model, and the retry guidance for each contract. `ToolSchema` provides the shape for tool argument extraction schemas, and the orchestrator ships helpers like `buildIntentClassificationPrompt`, `buildToolArgumentPrompt`, `buildStrictAnswerPrompt`, and `buildResponseFormattingPrompt` so callers do not need to re-implement the template logic. Each `validate*` helper feeds the raw model output through the contract validators before progressing.

## Runner

`assistant/src/runner.ts` glues the orchestrator to a model client (via the `LLMClient` interface) and replays the retry hints defined inside each contract. `Runner.executeContract` builds the `ModelInvocation`, waits for the client to return raw text, stores the result inside `AttemptRecord` entries, and re-renders the retry instructions before every new attempt using `getRetryInstruction`. That keeps the retry budget implicit, lets validators stay deterministic.

## Pipeline

`assistant/src/pipeline.ts` wires the decision pipeline together. It detects the user's language at the start, runs intent classification against tool-derived intents, extracts tool arguments when a `tool.*` intent is detected, executes the matching tool, and falls back to strict answers or the error channel when needed. Successful responses are optionally formatted as concise single sentences in the user's detected language via the response formatting contract before returning.

## Desktop App

`desktop` contains a Tauri/Vite UI that wires `Orchestrator`, `Runner`, and `OllamaClient` together. The renderer lets you type a question, pushes it through the strict answer contract, and renders each attempt so you can watch the retry guidance overlaying the result. The Rust backend (`desktop/src-tauri`) is a minimal host that simply launches the webview and exposes no custom commands.

To run the desktop experience:
1. Ensure Rust 1.72+ and the Tauri CLI are installed.
2. `cd desktop && npm install` (installs Vite and the @tauri-apps toolchain).
3. `npm run tauri` (launches the Vite dev server, then `tauri dev`).
4. `npm run tauri:build` after `npm run build` to produce bundles.

The UI expects Ollama to be listening on `http://127.0.0.1:11434` so `OllamaClient` can resolve the prompts generated by the orchestrator.

## Further Documentation

- [ARCHITECTURE](docs/ARCHITECTURE.md) - Overview of the system architecture and design principles.
